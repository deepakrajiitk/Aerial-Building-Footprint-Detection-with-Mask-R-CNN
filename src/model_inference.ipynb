{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and Defining Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings(action='ignore', category=DeprecationWarning, message='`np.bool` is a deprecated alias')\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-12 18:05:15.690512: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import fiona \n",
    "from shapely.geometry import shape, box\n",
    "import rasterio\n",
    "from PIL import Image, ImageDraw\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "import tensorflow as tf\n",
    "import imgaug.augmenters as iaa\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from patchify import patchify, unpatchify\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "import geopandas as gpd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "sys.path.append(os.path.join(ROOT_DIR, \"mrcnn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "DATASET_DIR = os.path.join(ROOT_DIR, \"Dataset\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = \"../models/mask_rcnn_coco.h5\"\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-12 18:05:18.031941: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2023-09-12 18:05:18.132498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:51:00.0 name: NVIDIA A30 computeCapability: 8.0\n",
      "coreClock: 1.44GHz coreCount: 56 deviceMemorySize: 23.50GiB deviceMemoryBandwidth: 869.04GiB/s\n",
      "2023-09-12 18:05:18.132540: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-09-12 18:05:18.151722: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2023-09-12 18:05:18.151817: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-09-12 18:05:18.154618: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2023-09-12 18:05:18.154902: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2023-09-12 18:05:18.155476: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2023-09-12 18:05:18.156218: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-09-12 18:05:18.156429: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/cctv/anaconda3/envs/mrcnn_env/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-09-12 18:05:18.156440: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig(Config):\n",
    "    NAME = \"BuildingDetection\"\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 4\n",
    "    STEPS_PER_EPOCH = 2770\n",
    "    VALIDATION_STEPS = 234\n",
    "    BACKBONE = \"resnet101\"\n",
    "    NUM_CLASSES = 2 #bulding and background\n",
    "    IMAGE_MAX_DIM = 1024\n",
    "    IMAGE_MIN_DIM = 1024\n",
    "\n",
    "    TRAIN_ROIS_PER_IMAGE = 50\n",
    "    \n",
    "    MAX_GT_INSTANCES = 50\n",
    "    LOSS_WEIGHTS = {\n",
    "        \"rpn_class_loss\": 2.,\n",
    "        \"rpn_bbox_loss\": 1.,\n",
    "        \"mrcnn_class_loss\": 1.,\n",
    "        \"mrcnn_bbox_loss\": 1.,\n",
    "        \"mrcnn_mask_loss\": 10.\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(TrainingConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    DETECTION_MIN_CONFIDENCE = 0.9\n",
    "\n",
    "inference_config = InferenceConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cctv/anaconda3/envs/mrcnn_env/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "WARNING:tensorflow:From /home/cctv/anaconda3/envs/mrcnn_env/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:602: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    }
   ],
   "source": [
    "model_inference = modellib.MaskRCNN(mode=\"inference\", config=inference_config, model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from  /home/cctv/Internship/Building_detection/logs/buildingdetection20230801T1248/mask_rcnn_buildingdetection_0043.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-12 18:05:24.153285: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-12 18:05:24.155762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-09-12 18:05:24.155776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      \n",
      "2023-09-12 18:05:24.458260: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2900000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-starting from epoch 43\n"
     ]
    }
   ],
   "source": [
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "model_path = os.path.join(MODEL_DIR, \"buildingdetection20230801T1248/mask_rcnn_buildingdetection_0043.h5\")\n",
    "# model_path = model_inference.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model_inference.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run inference on multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(image_path, patch_size, step):\n",
    "    \"\"\"\n",
    "    Extract patches from an image and return the patches along with a padded image.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "        patch_size (tuple): Size of each patch in (height, width) format.\n",
    "        step (int): Step size for patch extraction.\n",
    "\n",
    "    Returns:\n",
    "        patches (numpy.ndarray): Extracted patches.\n",
    "        padded_image (numpy.ndarray): Padded image to fit patches evenly.\n",
    "    \"\"\"\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Convert the PIL image to a NumPy array\n",
    "    image_np = np.array(image)\n",
    "    \n",
    "    # Calculate the number of patches in each dimension\n",
    "    num_patches_y = (image_np.shape[0] + patch_size[0] - 1) // patch_size[0]\n",
    "    num_patches_x = (image_np.shape[1] + patch_size[1] - 1) // patch_size[1]\n",
    "    \n",
    "    # Calculate the required padding to fit the patches evenly\n",
    "    pad_y = num_patches_y * patch_size[0] - image_np.shape[0]\n",
    "    pad_x = num_patches_x * patch_size[1] - image_np.shape[1]\n",
    "    \n",
    "    # Pad the image\n",
    "    padded_image = np.pad(image_np, ((0, pad_y), (0, pad_x), (0, 0)), mode='constant')\n",
    "    \n",
    "    # Divide the padded image into patches\n",
    "    patches = patchify(padded_image, patch_size, step=step)\n",
    "    \n",
    "    return patches, padded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_mask_on_edge(mask, edge_percentage, mask_percentage):\n",
    "    \"\"\"\n",
    "    Check if a mask is present on the edges of an image.\n",
    "\n",
    "    Args:\n",
    "        mask (numpy.ndarray): Binary mask.\n",
    "        edge_percentage (float): Percentage of edges to consider (0 to 1).\n",
    "        mask_percentage (float): Minimum percentage of mask pixels on an edge (0 to 1).\n",
    "\n",
    "    Returns:\n",
    "        bool: True if mask is on any edge, False otherwise.\n",
    "    \"\"\"\n",
    "    height, width = mask.shape\n",
    "    total_mask_area = np.sum(mask)\n",
    "    edge_pixels = int(min(height, width) * edge_percentage)\n",
    "\n",
    "    # Check top edge\n",
    "    if np.sum(mask[:edge_pixels, :]) >= mask_percentage * total_mask_area:\n",
    "        return True\n",
    "    \n",
    "    # Check bottom edge\n",
    "    if np.sum(mask[-edge_pixels:, :]) >= mask_percentage * total_mask_area:\n",
    "        return True\n",
    "    \n",
    "    # Check left edge\n",
    "    if np.sum(mask[:, :edge_pixels]) >= mask_percentage * total_mask_area:\n",
    "        return True\n",
    "    \n",
    "    # Check right edge\n",
    "    if np.sum(mask[:, -edge_pixels:]) >= mask_percentage * total_mask_area:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_polygons(r, x, y, eps = 0.02):\n",
    "    \"\"\"\n",
    "    Find polygons from instance segmentation results.\n",
    "\n",
    "    Args:\n",
    "        r (dict): Result dictionary containing masks and bounding boxes.\n",
    "        x (int): X-coordinate shift.\n",
    "        y (int): Y-coordinate shift.\n",
    "        eps (float, optional): Epsilon value for polygon approximation. \n",
    "            Controls the precision of the approximation. Default is 0.02.\n",
    "\n",
    "    Returns:\n",
    "        polygons (list): List of Shapely Polygon objects representing objects.\n",
    "    \"\"\"\n",
    "    masks = r['masks']\n",
    "    bboxes = r['rois']\n",
    "    n = bboxes.shape[0]\n",
    "    polygons = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        mask = masks[:, :, i]\n",
    "        mask = mask.astype(np.uint8)\n",
    "\n",
    "        # Ignore incomplete building masks on sides of images\n",
    "        if is_mask_on_edge(mask, 0.2, 0.5):\n",
    "            continue\n",
    "        \n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        for contour in contours:\n",
    "            epsilon = eps * cv2.arcLength(contour, True)\n",
    "            approx = cv2.approxPolyDP(contour, epsilon, closed=False)\n",
    "            \n",
    "            shifted_polygon = np.array(approx) + np.array([x, y])\n",
    "            \n",
    "            # Convert the shifted_polygon to a GeoDataFrame Polygon\n",
    "            poly_coords = [(point[0][0], point[0][1]) for point in shifted_polygon]\n",
    "            if len(poly_coords) >= 4:\n",
    "                polygon = Polygon(poly_coords)\n",
    "                polygons.append(polygon)\n",
    "            \n",
    "    return polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_patch_overlap_adjustment(polygons, prev_polygons, overlap_threshold):\n",
    "    adjusted_polygons = []\n",
    "    for polygon in polygons:\n",
    "        if not any(polygon.intersection(prev_poly).area >= overlap_threshold * polygon.area for prev_poly in prev_polygons):\n",
    "            adjusted_polygons.append(polygon)\n",
    "    return adjusted_polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_patches_with_overlap_adjustment(image_path, patches, model_inference, step, overlap_threshold, output_shp_path):\n",
    "    # Open the input image\n",
    "    image = Image.open(image_path)\n",
    "    image_width, image_height = image.size\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Extract CRS from the TIFF image\n",
    "    with rasterio.open(image_path) as src:\n",
    "        transform = src.transform\n",
    "        crs = src.crs\n",
    "    \n",
    "    # Initialize a 2D list to store polygons for each patch\n",
    "    patch_polygons = [[[] for _ in range(patches.shape[1])] for _ in range(patches.shape[0])]\n",
    "    \n",
    "    # Create a list to store all polygons for the shapefile\n",
    "    all_polygons = []\n",
    "\n",
    "    for i in range(patches.shape[0]):\n",
    "        for j in range(patches.shape[1]):\n",
    "            # Extract the current patch\n",
    "            patch = patches[i, j, 0, :, :, :3]\n",
    "            \n",
    "            # Perform model inference to detect objects in the patch\n",
    "            r = model_inference.detect([patch])[0]\n",
    "            \n",
    "            # Find polygons from the detection results and adjust for overlap\n",
    "            polygons = find_polygons(r, j * step, i * step)\n",
    "            \n",
    "            # Apply overlap adjustment with the previous row\n",
    "            if i > 0:\n",
    "                prev_row_polygons = patch_polygons[i - 1][j]\n",
    "                polygons = apply_patch_overlap_adjustment(polygons, prev_row_polygons, overlap_threshold)\n",
    "            \n",
    "            # Apply overlap adjustment with the previous column\n",
    "            if j > 0:\n",
    "                prev_col_polygons = patch_polygons[i][j - 1]\n",
    "                polygons = apply_patch_overlap_adjustment(polygons, prev_col_polygons, overlap_threshold)\n",
    "            \n",
    "            # Apply overlap adjustment with the diagonal patch (top-left)\n",
    "            if i > 0 and j > 0:\n",
    "                prev_diag_polygons = patch_polygons[i - 1][j - 1]\n",
    "                polygons = apply_patch_overlap_adjustment(polygons, prev_diag_polygons, overlap_threshold)\n",
    "            \n",
    "            # Apply overlap adjustment with the diagonal patch (top-right)\n",
    "            if i > 0 and j < patches.shape[1]-1:\n",
    "                prev_diag_polygons = patch_polygons[i - 1][j + 1]\n",
    "                polygons = apply_patch_overlap_adjustment(polygons, prev_diag_polygons, overlap_threshold)\n",
    "            \n",
    "            # Store the adjusted polygons in the 2D list\n",
    "            patch_polygons[i][j] = polygons\n",
    "\n",
    "            # Append the adjusted polygons to the list of all polygons\n",
    "            all_polygons.extend(polygons)\n",
    "            \n",
    "            # Print a message to indicate patch processing completion\n",
    "            print(f\"Patch {i},{j} completed\")\n",
    "\n",
    "    # Transform polygon coordinates from pixel to the TIFF file's coordinate system\n",
    "    transformed_polygons = []\n",
    "    for polygon in all_polygons:\n",
    "        points = []\n",
    "        for point in polygon.exterior.coords:\n",
    "            x, y = point\n",
    "            lon, lat = transform * (x + 0.5, y + 0.5)\n",
    "            points.append((lon, lat))\n",
    "        transformed_polygons.append(Polygon(points))\n",
    "\n",
    "    # Create a GeoDataFrame from the list of transformed polygons\n",
    "    geometry = gpd.GeoSeries(transformed_polygons)\n",
    "    print(len(all_polygons))\n",
    "    gdf = gpd.GeoDataFrame(geometry=geometry, crs=crs)  # You might need to adjust the CRS\n",
    "    \n",
    "    # Save the GeoDataFrame as a shapefile\n",
    "    gdf.to_file(output_shp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Gopepalli_geotiff_H1.tif\n",
      "Output path: ../results/Puttaparthi_Nallamada_Gopepalli_02627900/Gopepalli_geotiff_H1/output_Gopepalli_geotiff_H1.shp\n",
      "Working on Gopepalli_geotiff_H2.tif\n",
      "Output path: ../results/Puttaparthi_Nallamada_Gopepalli_02627900/Gopepalli_geotiff_H2/output_Gopepalli_geotiff_H2.shp\n",
      "Patch 0,0 completed\n",
      "Patch 0,1 completed\n",
      "Patch 0,2 completed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_221429/371406674.py\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# Process patches with overlap adjustment and save results as a shapefile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mprocess_patches_with_overlap_adjustment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_inference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlap_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_221429/1277857098.py\u001b[0m in \u001b[0;36mprocess_patches_with_overlap_adjustment\u001b[0;34m(image_path, patches, model_inference, step, overlap_threshold, output_shp_path)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# Perform model inference to detect objects in the patch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_inference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# Find polygons from the detection results and adjust for overlap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Internship/Building_detection/mrcnn/model.py\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(self, images, verbose)\u001b[0m\n\u001b[1;32m   2524\u001b[0m         \u001b[0;31m# Run object detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2525\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmrcnn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2526\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmolded_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_metas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2527\u001b[0m         \u001b[0;31m# Process detections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2528\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mrcnn_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_select_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m     return func.predict(\n\u001b[0m\u001b[1;32m    989\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mrcnn_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m     x, _, _ = model._standardize_user_data(\n\u001b[1;32m    702\u001b[0m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0;32m--> 703\u001b[0;31m     return predict_loop(\n\u001b[0m\u001b[1;32m    704\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mrcnn_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mrcnn_env/lib/python3.8/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4052\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4054\u001b[0;31m     fetched = self._callable_fn(*array_vals,\n\u001b[0m\u001b[1;32m   4055\u001b[0m                                 run_metadata=self.run_metadata)\n\u001b[1;32m   4056\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mrcnn_env/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1478\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m         \u001b[0mrun_metadata_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0m\u001b[1;32m   1481\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m                                                run_metadata_ptr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "overlap_threshold = 0.5  # Adjust this threshold as needed\n",
    "patch_size = (1024, 1024, 3)  # Adjust this to the desired patch size and channel count\n",
    "step = 512\n",
    "input_folder = \"../Dataset/test_files3/\"\n",
    "output_folder = \"../results\"\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Iterate over folders in the input directory\n",
    "for tif_folder in os.listdir(input_folder):\n",
    "    geotiff_folder = os.path.join(input_folder, tif_folder)\n",
    "    geotiff_folder = os.path.join(geotiff_folder, \"GeoTiff\")\n",
    "    curr_output_folder = os.path.join(output_folder, tif_folder)\n",
    "    \n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(curr_output_folder):\n",
    "        os.makedirs(curr_output_folder)\n",
    "\n",
    "    # Iterate over files in the GeoTiff folder\n",
    "    for filename in os.listdir(geotiff_folder):    \n",
    "        if filename.endswith(\".tif\"):\n",
    "            input_path = os.path.join(geotiff_folder, filename)\n",
    "            print(\"Working on\", filename)\n",
    "\n",
    "            inside_folder = os.path.join(curr_output_folder, f\"{filename[:-4]}\")\n",
    "            \n",
    "            # Create a subfolder for each input file\n",
    "            if not os.path.exists(inside_folder):\n",
    "                os.makedirs(inside_folder)\n",
    "                \n",
    "            output_path = os.path.join(inside_folder, f\"output_{filename[:-4]}\"+\".shp\")\n",
    "            print(\"Output path:\", output_path)\n",
    "            \n",
    "            # Skip processing if the output file already exists\n",
    "            if os.path.exists(output_path):\n",
    "                continue\n",
    "            \n",
    "            # Extract patches from the input image\n",
    "            patches, padded_image = extract_patches(input_path, patch_size, step)\n",
    "            \n",
    "            # Process patches with overlap adjustment and save results as a shapefile\n",
    "            process_patches_with_overlap_adjustment(input_path, patches, model_inference, step, overlap_threshold, output_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run inference on single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlap_threshold = 0.5  # Adjust this threshold as needed\n",
    "# image_path = \"../Dataset/test_files/Chowtakuntapalli_ORTHO_H1.tif\"  # Path to your test image\n",
    "# patch_size = (1024, 1024, 3)  # Adjust this to the desired patch size and channel count\n",
    "# step = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patches, padded_image = extract_patches(image_path, patch_size, step)\n",
    "# print(f\"Number of patches: {patches.shape[0] * patches.shape[1]}\")\n",
    "# print(f\"Shape of a single patch: {patches[0, 0, 0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_patches(patches, \"../Dataset/patches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_shp_path = \"../shp_files/output_shp.shp\"\n",
    "# process_patches_with_overlap_adjustment(image_path, patches, model_inference, step, overlap_threshold, output_shp_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrcnn_kernel",
   "language": "python",
   "name": "mrcnn_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
